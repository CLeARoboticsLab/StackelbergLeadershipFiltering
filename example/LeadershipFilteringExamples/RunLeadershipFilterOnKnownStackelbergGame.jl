# This file sets up a simple scenario and runs it on a Stackelberg game (with or without noise).
using StackelbergControlHypothesesFiltering

using Distributions
using LinearAlgebra
using Random
using Plots

include("leadfilt_LQ_parameters.jl")

# CONFIG: 
# We define an uncertainty for the measurements R arbitrarily - easy for now.
# 
rng = MersenneTwister(0)

R = zeros(xdim(dyn), xdim(dyn)) + 0.1 * I
zs = zeros(xdim(dyn), T)
Ts = 3
num_games = 1

p = 0.6
p_init = 0.6


# Solve an LQ Stackelberg game based on the shepherd and sheep example.
Ps_strategies, Zs_future_costs = solve_lq_stackelberg_feedback(dyn, costs, T+Ts-1, leader_idx)
xs, us = unroll_feedback(dyn, times, Ps_strategies, x‚ÇÅ)


# Fill in z as noisy state measurements.
for tt in 1:T
    zs[:, tt] = rand(rng, MvNormal(xs[:, tt], R))
end


# TODO(hamzah) - vectorize this better
function generate_discrete_state_transition(p‚ÇÅ‚ÇÅ, p‚ÇÇ‚ÇÇ)

    distribs = [Bernoulli(p‚ÇÅ‚ÇÅ), Bernoulli(p‚ÇÇ‚ÇÇ)]

    # state transition matrix of state
    P = [ p‚ÇÅ‚ÇÅ  1-p‚ÇÇ‚ÇÇ;
         1-p‚ÇÅ‚ÇÅ  p‚ÇÇ‚ÇÇ]

    # The discrete state transition stays in state i with probability p·µ¢.
    function discrete_state_transition(time_range, s_prev, s_probs, ùí≥_prev, s_actions, rng)

        @assert length(s_prev) == 1
        s_prev = s_prev[1]
        sample = rand(rng, distribs[s_prev], 1)

        # use markov chain to adjust over time
        other_state = (s_prev == 1) ? 2 : 1
        s_new = (isone(sample[1])) ? s_prev : other_state

        return [s_new]
    end
    return discrete_state_transition, P
end

discrete_state_transition, state_trans_P = generate_discrete_state_transition(p, 1-p)
s_init_distrib = Bernoulli(p_init)


xÃÇs, PÃÇs, probs, sg_objs = leadership_filter(dyn, costs, t0, times,
                           T,         # simulation horizon
                           Ts,        # horizon over which the stackelberg game should be played,
                           num_games, # number of stackelberg games played for measurement
                           x‚ÇÅ,        # initial state at the beginning of simulation
                           P‚ÇÅ,        # initial covariance at the beginning of simulation
                           us,        # the control inputs that the actor takes
                           zs,        # the measurements
                           R,
                           s_init_distrib,
                           discrete_state_transition;
                           threshold=1.,
                           rng,
                           max_iters=1000,
                           step_size=0.01,
                           Ns=10,
                           verbose=false)

# TODO(hamzah) - Next step, make these plots better so I can debug what's going on.
plot(times[1:T], probs, xlabel="t (s)", ylabel="prob. leadership")
